# =============================================================================
# DeepASM-Net / Wavefield Dehazing 默认配置文件
# =============================================================================

# 随机种子，用于复现实验
seed: 123

# -----------------------------------------------------------------------------
# 数据路径与数据集配置
# -----------------------------------------------------------------------------
paths:
  # 数据根目录，所有 hazy_dir / clean_dir 均相对于此路径
  root: D:\Desktop\毕业论文\code\data
  # root: D:\声呐项目\qwh工作区


  # 是否使用旧版 hazy/clean 单一路径配置
  use_legacy_hazy_clear: false

  # 是否自动发现 root 下所有 IN/GT 子目录作为数据集
  auto_discover_in_gt: false

  # 训练集列表，可配置多个数据集
  # type 可选: same_name（文件名完全相同）, reside, hazy_clear
  # - same_name: hazy 与 GT 文件名一一对应（你的 RESIDE-IN train 为此格式）
  # - reside: hazy 100_6.png 对应 GT 100_6_0.xx.png（另一种 RESIDE 格式）
  datasets:
    - type: same_name
      hazy_dir: RESIDE-IN/train/hazy
      clean_dir: RESIDE-IN/train/GT

  # 测试集（仅当 split.use_native_split=true 时使用）
  # 使用数据集官方的 train/test 划分，验证 PSNR/SSIM 在 test 上计算
  test_datasets:
    - type: same_name
      hazy_dir: RESIDE-IN/test/hazy
      clean_dir: RESIDE-IN/test/GT

# -----------------------------------------------------------------------------
# 数据划分策略
# -----------------------------------------------------------------------------
split:
  # 是否使用数据集自带的 train/test 划分
  # true: 训练用 datasets，验证用 test_datasets
  # false: 只用 datasets，按 train_ratio 随机划分 train/val
  use_native_split: true

  # 当 use_native_split=false 时，训练集占全部数据的比例
  train_ratio: 0.85

  # 划分时是否打乱顺序
  shuffle: true

# -----------------------------------------------------------------------------
# DataLoader 参数
# -----------------------------------------------------------------------------
loader:
  batch_size: 8          # 训练 batch 大小，显存不足可减小（如 4）
  val_batch_size: 8      # 验证 batch 大小
  num_workers: 8         # 数据加载进程数，建议为 CPU 核数的一半或相等
  pin_memory: true       # 锁页内存，加速 CPU→GPU 数据传输
  prefetch_factor: 4     # 每个 worker 预取的 batch 数，减少 GPU 空闲等待
  persistent_workers: true  # 是否复用 worker，避免每个 epoch 重新创建

# -----------------------------------------------------------------------------
# 数据增强与预处理
# -----------------------------------------------------------------------------
transforms:
  image_size: 256        # 输入图像裁剪/缩放到此尺寸
  random_flip: true      # 训练时随机水平翻转
  use_cv2_load: true     # 用 cv2 读图（通常比 PIL 快，且支持中文路径）

# -----------------------------------------------------------------------------
# 模型结构
# -----------------------------------------------------------------------------
model:
  num_layers: 1          # ABlock 堆叠层数（1=单块实验，无 Mix 头/通道注意力）
  share_weights: false   # 各层 ABlock 是否共享权重
  use_skip_fusion: true  # 是否使用 U-Net 式跳跃连接 + SK Fusion
  use_norm: true         # 是否使用归一化层
  use_mix_head: true    # 实验用：false=去掉 Mix 头与通道注意力，直接输出 ASM 的 J

  # 调试：单独禁用某模块，用于排查是距离预测还是相位预测在起作用
  disable_z_module: false      # true=禁用距离预测，写死 z=z_max/2
  disable_phase_module: false  # true=禁用相位预测，写死 phi=0

  # 可选：在 ABlock 内加入空间 Transformer，为相位/z 预测提供全局上下文
  transformer:
    enabled: false
    embed_dim: 48
    num_heads: 4
    scale: 8        # 空间下采样倍数，序列长 (H/8)*(W/8)，越大越省显存
    dropout: 0.0

  # 波长 (μm)，对应 R/G/B 三通道，用于 ASM 传播
  wavelengths: [0.65, 0.53, 0.47]

  # 传播距离 z 的最大值（归一化）
  z_max: 0.3

  # ASM 输出模式: identity / abs / abs2
  amp_mode: identity
  output_mode: abs       # 输出 |Uz| 或 |Uz|^2

  mix_hidden: 32         # Mix 头隐藏通道数（64→32 减参）

  # 相位预测子网络
  phase_module:
    hidden: 16            # 32→16 减参
    num_layers: 1         # 2→1 减参
    use_channel_residual: true
    phase_residual_scale: 0.6

  # 传播距离 z 预测子网络
  z_module:
    hidden: 16             # 32→16 减参
    num_layers: 1          # 2→1 减参

  # 颜色校正模块（单块实验时可关闭）
  color_corrector:
    enabled: false         # 单块实验关闭
    hidden: 16

# -----------------------------------------------------------------------------
# 训练超参数
# -----------------------------------------------------------------------------
train:
  epochs: 50            # 训练轮数
  lr: 1.0e-4             # 学习率
  weight_decay: 1.0e-4   # 权重衰减（L2 正则）
  cosine_tmax: 100       # 余弦退火调度器的周期
  warmup_epochs: 5       # 学习率 warmup 轮数
  grad_clip: 1.0         # 梯度裁剪阈值，0 表示不裁剪
  amp: true              # 是否使用混合精度 (FP16) 加速
  log_interval: 50       # 每多少 step 打印一次（若支持）
  fixed_val_samples: 12  # 每轮保存的验证样本数量（用于生成对比图）

# -----------------------------------------------------------------------------
# 损失函数权重
# -----------------------------------------------------------------------------
# L1 = mean(|pred - clean|)，逐像素绝对误差，数值稳定、对 PSNR 也有帮助。
# PSNR 定义是 10*log10(1/MSE)，即与 MSE(L2) 直接对应；若只冲 PSNR 可只留 l1 或只留 l2。
# 下面为「只冲 PSNR」预设：仅开 l1，其余为 0；若要和 PSNR 完全对齐可设 l1: 0, l2: 1.0。
loss:
  l1: 0.0                # L1 重建损失 mean(|pred-clean|)
  l2: 1.0                # L2/MSE 损失，与 PSNR 直接对应，只冲 PSNR 可设 l2: 1.0、l1: 0
  ssim: 0.0              # SSIM 损失（1 - SSIM）
  perceptual: 0.0       # VGG 感知损失
  color: 0.0             # 色度损失 (YCbCr)
  luma: 0.0              # 亮度损失
  edge: 0.0             # 边缘损失 (Sobel)
  saturation: 0.0        # 饱和度损失
  z_reg: 0.0             # z 通道正则（三通道 z 差异）
  phase_tv: 0.0         # 相位 Total Variation 正则
  perceptual_layers: [3, 8, 15]  # VGG 感知损失使用的层索引（perceptual>0 时有效）

# -----------------------------------------------------------------------------
# 输出目录
# -----------------------------------------------------------------------------
output:
  runs_dir: ./runs       # 训练 run 保存根目录（含 checkpoint、日志、样本图）
  outputs_dir: ./outputs # 推理输出目录（infer.py 等使用）
